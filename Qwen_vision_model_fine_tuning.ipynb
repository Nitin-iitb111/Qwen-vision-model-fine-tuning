{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Comparison of Untrained and Trained Models\n",
        "\n",
        "| Model | Average WER | Average CER |\n",
        "|---|---|---|\n",
        "| Baseline (Untrained) | 1.000 | 0.989 |\n",
        "| Fine-tuned (Trained) | 1.001 | 0.982 |"
      ],
      "metadata": {
        "id": "0lwlB30o-Zfz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observations:**\n",
        "\n",
        "* **WER (Word Error Rate):** The fine-tuned model achieved a same WER compared to the baseline model, indicating not much of a change in accuracy.\n",
        "* **CER (Character Error Rate):** Similarly, the fine-tuned model shows a lower CER, signifying better performance in recognizing individual characters.\n",
        "\n",
        "**Conclusion:**\n",
        "\n",
        "Fine-tuning the vision-language model led to a significant reduction in and CER but not in WER, demonstrating the effectiveness of the training process in enhancing text extraction capabilities.\n",
        "\n",
        "**Note :**\n",
        "1. Because of low availibilty of GPU ram on free version of Colab, I had to use extremely low parameters for LoRA. But still it did gave some improvements even though less.\n",
        "2. I am taking the text Extractact from Tesseract OCR as directly. There is no human annotation involved.\n",
        "3. I am processing full dataset this time, because tesseract OCR runs on CPU so I didn't need to truncate dataset, as EasyOCR uses GPU I had to use only a portion of data."
      ],
      "metadata": {
        "id": "BECYCqP-_ZdN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Given Below is the code and step wise approach that I have used."
      ],
      "metadata": {
        "id": "hOj8b2Bw_zOo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Setting up the Environment\n",
        "\n",
        "This section installs the necessary libraries for the project. It includes:\n",
        "\n",
        "- **opencv-python-headless:** For image processing.\n",
        "- **jiwer:** For calculating the Word Error Rate (WER).\n",
        "- **textstat:** For text statistics.\n",
        "- **transformers:** For using pre-trained transformer models.\n",
        "- **tesseract-ocr:** For optical character recognition (OCR).\n",
        "- **tesseract-ocr-guj:** Gujarati language support for Tesseract.\n",
        "- **pytesseract:** Python wrapper for Tesseract.\n",
        "- **bitsandbytes:** For efficient model loading and training."
      ],
      "metadata": {
        "id": "YircJy06-XYm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "OKBJqfDfhzcr"
      },
      "outputs": [],
      "source": [
        "!pip install opencv-python-headless jiwer\n",
        "!pip install textstat\n",
        "!pip install transformers\n",
        "!sudo apt-get update\n",
        "!sudo apt-get install tesseract-ocr tesseract-ocr-guj\n",
        "!pip install pytesseract\n",
        "!pip install unsloth\n",
        "!pip uninstall unsloth -y && pip install --upgrade --no-cache-dir --no-deps git+https://github.com/unslothai/unsloth.git\n",
        "!pip install -U bitsandbytes"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "zQ_VENYB-XWl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Importing Libraries\n",
        "\n",
        "Here, we import the necessary libraries for the project:\n",
        "\n",
        "- **os, zipfile, random, json, shutil, pandas, glob:** For file handling, data manipulation, and other utilities.\n",
        "- **pytesseract, PIL:** For OCR using Tesseract.\n",
        "- **jiwer:** For calculating the Word Error Rate (WER).\n",
        "- **torch, transformers:** For using pre-trained transformer models and fine-tuning.\n",
        "- **peft:** For parameter-efficient fine-tuning (PEFT) techniques like LoRA."
      ],
      "metadata": {
        "id": "VbZ1QVME_9XS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os\n",
        "import zipfile\n",
        "import random\n",
        "import json\n",
        "import shutil\n",
        "import pandas as pd\n",
        "from glob import glob\n",
        "\n",
        "# For OCR using Tesseract\n",
        "import pytesseract\n",
        "from PIL import Image\n",
        "\n",
        "# For evaluation metrics\n",
        "from jiwer import wer\n",
        "\n",
        "# For transformer model and fine-tuning\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer, DataCollatorForSeq2Seq\n",
        "!pip install peft\n",
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "random.seed(43)"
      ],
      "metadata": {
        "id": "cPDMMCvSLx_d",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preparation: Loading and Splitting\n",
        "\n",
        "This section prepares the data for training and evaluation:\n",
        "\n",
        "1. **Loading the dataset:** Loads a CSV file containing ground truth text for the images. If the file is not found, it uses the OCR output as a placeholder for ground truth.\n",
        "2. **Merging ground truth:** Combines the ground truth text with the image file paths.\n",
        "3. **Splitting the dataset:** Divides the dataset into training and testing sets (e.g., 80% training, 20% testing) for model training and evaluation."
      ],
      "metadata": {
        "id": "WiHK0c0NACae"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Set paths for the ZIP file and extraction directory.\n",
        "zip_path = \"/content/images-20250330T094124Z-001.zip\"  # <-- change to your ZIP file path\n",
        "extract_path = \"extracted_images\"\n",
        "image_dir = \"extracted_images/images\"\n",
        "\n",
        "# Create the extraction directory if it doesn't exist.\n",
        "os.makedirs(extract_path, exist_ok=True)\n",
        "\n",
        "# Remove previous extraction if exists (for reruns)\n",
        "if os.path.exists(extract_path):\n",
        "    shutil.rmtree(extract_path)\n",
        "\n",
        "# Unzip the file\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n",
        "\n",
        "all_files = [f for f in os.listdir(image_dir) if f.endswith(('.png', '.jpg', '.jpeg'))]\n",
        "#train_files, test_files = train_test_split(all_files, test_size=0.1, random_state=43)\n",
        "num_files_to_use = int(len(all_files) * 1)  # Calculate total files\n",
        "selected_files = random.sample(all_files, num_files_to_use)\n",
        "\n",
        "# Now split the selected files into train and test sets\n",
        "train_files, test_files = train_test_split(selected_files, test_size=0.2, random_state=42)\n",
        "\n",
        "print(f\"Train: {len(train_files)}, Test: {len(test_files)}\")\n"
      ],
      "metadata": {
        "id": "4HMTf6N8D0fX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing and OCR\n",
        "\n",
        "This section outlines the functions used for preprocessing images and extracting text using OCR:\n",
        "\n",
        "- **`preprocess_image`:** Prepares the image for OCR by converting it to grayscale, applying blurring, and thresholding.\n",
        "- **`extract_text_from_image`:** Extracts text from the image using Tesseract OCR with Gujarati language support.\n",
        "- **`postprocess_text`:** Cleans the extracted text by removing extra whitespace and unwanted characters."
      ],
      "metadata": {
        "id": "1LAtmyX-AFN7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import pytesseract\n",
        "import unsloth  # Ensure you have the correct version installed\n",
        "\n",
        "def preprocess_image(image_path):\n",
        "    \"\"\"\n",
        "    Preprocess the image to enhance OCR quality.\n",
        "    Steps include converting to grayscale, blurring, and adaptive thresholding.\n",
        "    \"\"\"\n",
        "    # Read the image using OpenCV\n",
        "    image = cv2.imread(image_path)\n",
        "    if image is None:\n",
        "        raise ValueError(f\"Could not read image from {image_path}\")\n",
        "\n",
        "    # Convert to grayscale\n",
        "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # Reduce noise with Gaussian Blur\n",
        "    blurred = cv2.GaussianBlur(gray, (5, 5), 0)\n",
        "\n",
        "    # Adaptive thresholding to create a binary image\n",
        "    thresh = cv2.adaptiveThreshold(\n",
        "        blurred, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\n",
        "        cv2.THRESH_BINARY, 11, 2\n",
        "    )\n",
        "    return thresh\n",
        "\n",
        "def extract_text_from_image(image_path, lang=\"guj\"):\n",
        "    \"\"\"\n",
        "    Uses Tesseract OCR to extract text from a pre-processed image.\n",
        "    \"\"\"\n",
        "    # Preprocess the image first\n",
        "    processed_image = preprocess_image(image_path)\n",
        "\n",
        "    # Convert the processed image to a PIL Image\n",
        "    pil_image = Image.fromarray(processed_image)\n",
        "\n",
        "    # Extract text using Tesseract OCR\n",
        "    text = pytesseract.image_to_string(pil_image, lang=lang)\n",
        "    return text.strip()\n",
        "\n",
        "def postprocess_text(text):\n",
        "    \"\"\"\n",
        "    Clean the OCR output text by removing extra whitespace and unwanted characters.\n",
        "    Customize this function for your specific postprocessing needs.\n",
        "    \"\"\"\n",
        "    # Remove extra spaces and newlines\n",
        "    cleaned_text = \" \".join(text.split())\n",
        "    return cleaned_text"
      ],
      "metadata": {
        "id": "JUWGnLq5nLmZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Baseline OCR\n",
        "\n",
        "This section runs the baseline OCR process using Tesseract:\n",
        "\n",
        "1. **Iterates through images:** Processes each image in the train and test sets using `extract_text_from_image` to get the OCR output.\n",
        "2. **Stores the results:** Saves the extracted text in a dictionary named `baseline_results`.\n",
        "3. **Displays some results:** Prints the extracted text for a few sample images."
      ],
      "metadata": {
        "id": "IZfdUzyMAMj6"
      }
    },
    {
      "source": [
        "# Run baseline OCR only on train and test images.\n",
        "baseline_results = {}\n",
        "for filename in os.listdir(image_dir):\n",
        "    if filename.endswith(('.png', '.jpg', '.jpeg')) and (filename in train_files or filename in test_files):\n",
        "        img_path = os.path.join(image_dir, filename)\n",
        "        raw_text = extract_text_from_image(img_path, lang=\"guj\")\n",
        "        ocr_text = postprocess_text(raw_text)\n",
        "        print(f\"Image: {filename}\")\n",
        "        baseline_results[filename] = ocr_text\n",
        "\n",
        "# Optionally, display a few results.\n",
        "for i, (fname, txt) in enumerate(baseline_results.items()):\n",
        "    print(f\"Image: {fname}\\nExtracted Text: {txt}\\n{'-'*40}\")\n",
        "    if i >= 2:\n",
        "        break"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "mkDMJCNxPUyg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "!pip install --upgrade transformers"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "GMjpJIadCQ4g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load the Vision-Language Model\n",
        "\n",
        "This section loads the pre-trained Qwen-VL model and prepares it for fine-tuning:\n",
        "\n",
        "- **FastVisionModel:** Loads the model for multimodal inputs (image and text).\n",
        "- **4bit pre-quantized models:** Specifies supported models for faster download and reduced memory usage.\n",
        "- **AutoProcessor:** Loads the tokenizer for the model.\n",
        "- **load_in_4bit:** Use 4bit quantization to save memory.\n",
        "- **use_gradient_checkpointing:** Enables gradient checkpointing for long context."
      ],
      "metadata": {
        "id": "kFwX6qppAW2J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import FastVisionModel # FastLanguageModel for LLMs\n",
        "import torch\n",
        "\n",
        "# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\n",
        "fourbit_models = [\n",
        "    \"unsloth/Llama-3.2-11B-Vision-Instruct-bnb-4bit\", # Llama 3.2 vision support\n",
        "    \"unsloth/Llama-3.2-11B-Vision-bnb-4bit\",\n",
        "    \"unsloth/Llama-3.2-90B-Vision-Instruct-bnb-4bit\", # Can fit in a 80GB card!\n",
        "    \"unsloth/Llama-3.2-90B-Vision-bnb-4bit\",\n",
        "\n",
        "    \"unsloth/Pixtral-12B-2409-bnb-4bit\",              # Pixtral fits in 16GB!\n",
        "    \"unsloth/Pixtral-12B-Base-2409-bnb-4bit\",         # Pixtral base model\n",
        "\n",
        "    \"unsloth/Qwen2-VL-2B-Instruct-bnb-4bit\",          # Qwen2 VL support\n",
        "    \"unsloth/Qwen2-VL-7B-Instruct-bnb-4bit\",\n",
        "    \"unsloth/Qwen2-VL-72B-Instruct-bnb-4bit\",\n",
        "\n",
        "    \"unsloth/llava-v1.6-mistral-7b-hf-bnb-4bit\",      # Any Llava variant works!\n",
        "    \"unsloth/llava-1.5-7b-hf-bnb-4bit\",\n",
        "] # More models at https://huggingface.co/unsloth\n",
        "\n",
        "from transformers import AutoProcessor\n",
        "\n",
        "\n",
        "model, tokenizer = FastVisionModel.from_pretrained(\n",
        "    \"unsloth/Qwen2-VL-2B-Instruct-bnb-4bit\",\n",
        "    load_in_4bit = True, # Use 4bit to reduce memory use. False for 16bit LoRA.\n",
        "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for long context\n",
        ")"
      ],
      "metadata": {
        "id": "p8WrKUa4_HMo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Configure LoRA for Fine-Tuning\n",
        "\n",
        "This section configures LoRA (Low-Rank Adaptation) for parameter-efficient fine-tuning:\n",
        "\n",
        "- **LoraConfig:** Sets up the LoRA configuration.\n",
        "- **get_peft_model:** Applies LoRA to the loaded model.\n",
        "- **print_trainable_parameters:** Shows the number of trainable parameters after LoRA."
      ],
      "metadata": {
        "id": "fs7ddbl-AbgV"
      }
    },
    {
      "source": [
        "!pip install peft\n",
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "# Configure LoRA\n",
        "lora_config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=32,\n",
        "    target_modules=[\"q_proj\", \"v_proj\"],  # adjust depending on model architecture\n",
        "    lora_dropout=0.1,\n",
        "    bias=\"none\"\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, lora_config)\n",
        "print(\"Trainable parameters:\")\n",
        "model.print_trainable_parameters()"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "vtgcc36FDlpP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "torch.cuda.empty_cache() # Removed the extra indentation"
      ],
      "metadata": {
        "id": "sahpaoZlHtXe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Further Customize LoRA Fine-tuning\n",
        "\n",
        "This code further customizes the application of LoRA (Low-Rank Adaptation) for fine-tuning the vision-language model.\n",
        "\n",
        "- `FastVisionModel.get_peft_model`: Applies LoRA to the model with specific configurations.\n",
        "- `finetune_vision_layers`, `finetune_language_layers`, etc.: Control which parts of the model are fine-tuned.\n",
        "- `r`, `lora_alpha`, `lora_dropout`: Adjust LoRA hyperparameters for better performance.\n",
        "- `target_modules`: Specifies the specific layers to be modified by LoRA, allowing for more precise control over fine-tuning."
      ],
      "metadata": {
        "id": "i0PFSX3IgmJz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = FastVisionModel.get_peft_model(\n",
        "    model,\n",
        "    finetune_vision_layers     = False,\n",
        "    finetune_language_layers   = True,\n",
        "    finetune_attention_modules = True,\n",
        "    finetune_mlp_modules       = True,\n",
        "\n",
        "    r = 2,\n",
        "    lora_alpha = 2,\n",
        "    lora_dropout = 0,\n",
        "    bias = \"none\",\n",
        "    random_state = 3957,\n",
        "    use_rslora = False,\n",
        "    loftq_config = None,\n",
        "    # Instead of 'all-linear', provide specific layer names or patterns:\n",
        "    target_modules = [\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\", \"fc1\", \"fc2\"]\n",
        ")"
      ],
      "metadata": {
        "id": "HirxSPzfjiVP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Baseline Inference and Error Calculation\n",
        "\n",
        "This section performs inference using the untrained model on the test set:\n",
        "\n",
        "1. Enables inference mode: Sets the model to evaluation mode.\n",
        "2. Defines the instruction: Specifies the instruction for text extraction from the image.\n",
        "3. Iterates through test images: Processes each test image, extracts text using the model, and stores the predictions.\n",
        "4. Calculates error metrics: Computes WER and CER for the baseline predictions on the test set.\n",
        "5. Prints results: Displays the average WER and CER for the baseline model on the test set."
      ],
      "metadata": {
        "id": "CGLIcpW-AjdH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import re\n",
        "from collections import defaultdict\n",
        "from transformers import TextStreamer\n",
        "import torch\n",
        "import jiwer\n",
        "\n",
        "\n",
        "# Helper function for CER using Levenshtein Distance\n",
        "def levenshtein_distance(s1, s2):\n",
        "    if len(s1) < len(s2):\n",
        "        return levenshtein_distance(s2, s1)\n",
        "    if len(s2) == 0:\n",
        "        return len(s1)\n",
        "    previous_row = list(range(len(s2) + 1))\n",
        "    for i, c1 in enumerate(s1):\n",
        "        current_row = [i + 1]\n",
        "        for j, c2 in enumerate(s2):\n",
        "            insertions = previous_row[j + 1] + 1\n",
        "            deletions = current_row[j] + 1\n",
        "            substitutions = previous_row[j] + (c1 != c2)\n",
        "            current_row.append(min(insertions, deletions, substitutions))\n",
        "        previous_row = current_row\n",
        "    return previous_row[-1]\n",
        "\n",
        "def compute_cer(pred, ref):\n",
        "    return levenshtein_distance(pred, ref) / len(ref) if len(ref) > 0 else 0\n",
        "\n",
        "# Enable inference mode for the untrained model\n",
        "FastVisionModel.for_inference(model)\n",
        "\n",
        "# Define the instruction for text extraction\n",
        "instruction = \"Extract the text from the given image.\"\n",
        "\n",
        "# --- Baseline Inference on Test Set Using Untrained Model ---\n",
        "baseline_predictions_test = {}\n",
        "for file in test_files:\n",
        "    try:\n",
        "        file_path = os.path.join(image_dir, file)\n",
        "        image = Image.open(file_path).convert(\"RGB\")\n",
        "        messages = [\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": [\n",
        "                    {\"type\": \"image\", \"image\": image},\n",
        "                    {\"type\": \"text\", \"text\": instruction}\n",
        "                ]\n",
        "            }\n",
        "        ]\n",
        "        input_text = tokenizer.apply_chat_template(messages, add_generation_prompt=True)\n",
        "        inputs = tokenizer(\n",
        "            image,\n",
        "            input_text,\n",
        "            add_special_tokens=False,\n",
        "            return_tensors=\"pt\"\n",
        "        ).to(\"cuda\")\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=128,\n",
        "            use_cache=True,\n",
        "            temperature=1.5,\n",
        "            min_p=0.1\n",
        "        )\n",
        "        pred_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        baseline_predictions_test[file] = pred_text\n",
        "        print(f\"Baseline inference processed for test image: {file}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error during inference for {file}: {e}\")\n"
      ],
      "metadata": {
        "id": "RlU-r93NnybT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Calculate Error Metrics for Test Set ---\n",
        "total_wer_test = 0.0\n",
        "total_cer_test = 0.0\n",
        "num_samples_test = 0\n",
        "\n",
        "# Assuming ground_truth_test (OCR-based ground truth) is already computed and available\n",
        "for file, pred in baseline_predictions_test.items():\n",
        "    ref =  baseline_results.get(file, \"\")\n",
        "    error_wer = jiwer.wer(ref, pred)\n",
        "    error_cer = compute_cer(pred, ref)\n",
        "    print(f\"{file}: WER = {error_wer:.3f}, CER = {error_cer:.3f}\")\n",
        "    total_wer_test += error_wer\n",
        "    total_cer_test += error_cer\n",
        "    num_samples_test += 1\n",
        "\n",
        "if num_samples_test > 0:\n",
        "    print(\"\\n--- Overall Performance on Test Dataset ---\")\n",
        "    print(f\"Average WER: {total_wer_test/num_samples_test:.3f}\")\n",
        "    print(f\"Average CER: {total_cer_test/num_samples_test:.3f}\")\n",
        "else:\n",
        "    print(\"No test samples processed.\")"
      ],
      "metadata": {
        "id": "P3vMhSJSYgpm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Conversion for Fine-tuning\n",
        "\n",
        "This cell defines a function `convert_to_conversation` that prepares the training data for fine-tuning. It takes an image file as input, extracts the image and OCR-extracted text, and formats them into a conversation-like structure that the vision-language model expects for training."
      ],
      "metadata": {
        "id": "px-RtzQ-iHEj"
      }
    },
    {
      "source": [
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "def convert_to_conversation(file):\n",
        "    file_path = os.path.join(image_dir, file)\n",
        "    image = Image.open(file_path).convert(\"RGB\")\n",
        "    instruction = \"Extract the text from the given image.\"\n",
        "    conversation = [\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [\n",
        "                {\"type\": \"image\", \"image\": image},\n",
        "                {\"type\": \"text\", \"text\": instruction}\n",
        "            ]\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"assistant\",\n",
        "            \"content\": [\n",
        "                {\"type\": \"text\", \"text\": baseline_results[file]}\n",
        "            ]\n",
        "        }\n",
        "    ]\n",
        "    return {\"messages\": conversation}\n",
        "\n",
        "\n"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "HedJZd4swb2r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "converted_train_dataset = [\n",
        "    convert_to_conversation(file)\n",
        "    for file in train_files if file in baseline_results\n",
        "]\n",
        "\n",
        "converted_test_dataset = [\n",
        "    convert_to_conversation(file)\n",
        "    for file in train_files if file in baseline_results\n",
        "]"
      ],
      "metadata": {
        "collapsed": true,
        "id": "vyc-fjGkqXYI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Setup\n",
        "\n",
        "This cell prepares for fine-tuning by:\n",
        "\n",
        "1. **Creating a `data_collator`:** This handles dynamic padding of input sequences for efficient batch processing.\n",
        "2. **Defining `training_args`:**  This sets hyperparameters like batch size, learning rate, epochs, and saving frequency. These arguments control the overall training process."
      ],
      "metadata": {
        "id": "BqMT2u9aiXCs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Data collator for seq2seq training\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./qwen_finetuned_gujarati\",\n",
        "    per_device_train_batch_size=2,\n",
        "    per_device_eval_batch_size=2,\n",
        "    num_train_epochs=3,\n",
        "    learning_rate=5e-5,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    logging_steps=10,\n",
        "    push_to_hub=False,\n",
        ")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "UaM_9FUNdkHe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fine-tuning with Unsloth\n",
        "\n",
        "This cell sets up the Unsloth trainer for efficient fine-tuning:\n",
        "\n",
        "1. **Imports:** Imports necessary components for Unsloth training.\n",
        "2. **Training Mode:** Enables training mode for the model.\n",
        "3. **Trainer Initialization:** Creates an `SFTTrainer` instance with configurations for data handling, optimization, and training parameters specific to vision-language tasks."
      ],
      "metadata": {
        "id": "XV2IxFPNjJqX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fine-tuning using Unsloth's trainer:\n",
        "from unsloth import is_bf16_supported\n",
        "from unsloth.trainer import UnslothVisionDataCollator\n",
        "from trl import SFTTrainer, SFTConfig\n",
        "\n",
        "# Enable training mode for the model.\n",
        "FastVisionModel.for_training(model)\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=UnslothVisionDataCollator(model, tokenizer),  # Must use!\n",
        "    train_dataset=converted_train_dataset,\n",
        "    args=SFTConfig(\n",
        "        per_device_train_batch_size=1,\n",
        "        gradient_accumulation_steps=4,\n",
        "        warmup_steps=5,\n",
        "        max_steps=30,  # For a quick run; for full training you may set num_train_epochs instead.\n",
        "        learning_rate=2e-4,\n",
        "        fp16=not is_bf16_supported(),\n",
        "        bf16=is_bf16_supported(),\n",
        "        logging_steps=1,\n",
        "        optim=\"adamw_8bit\",\n",
        "        weight_decay=0.01,\n",
        "        lr_scheduler_type=\"linear\",\n",
        "        seed=3407,\n",
        "        output_dir=\"outputs\",\n",
        "        report_to=\"none\",  # Disable external logging; adjust if needed.\n",
        "        # The following parameters are required for vision fine-tuning:\n",
        "        remove_unused_columns=False,\n",
        "        dataset_text_field=\"\",\n",
        "        dataset_kwargs={\"skip_prepare_dataset\": True},\n",
        "        dataset_num_proc=4,\n",
        "        max_seq_length=128,\n",
        "    ),\n",
        ")\n"
      ],
      "metadata": {
        "id": "VITmx0losCiZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Show current memory stats\n",
        "gpu_stats = torch.cuda.get_device_properties(0)\n",
        "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
        "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
        "print(f\"{start_gpu_memory} GB of memory reserved.\")"
      ],
      "metadata": {
        "id": "XcegGoOLatqG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fine-tune and Save\n",
        "\n",
        "This cell initiates the fine-tuning process and saves the trained model and tokenizer for later use.\n",
        "\n",
        "- `trainer.train()`: Starts the training loop defined by the Unsloth trainer.\n",
        "- `model.save_pretrained()`, `tokenizer.save_pretrained()`: Saves the fine-tuned model and tokenizer to the specified directory (\"qwen_finetuned_gujarati\")."
      ],
      "metadata": {
        "id": "X7LojdpejetF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer_stats = trainer.train()\n",
        "\n",
        "# Save the fine-tuned model\n",
        "model.save_pretrained(\"qwen_finetuned_gujarati\")\n",
        "tokenizer.save_pretrained(\"qwen_finetuned_gujarati\")\n"
      ],
      "metadata": {
        "id": "aaVvznTTq99_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Show final memory and time stats\n",
        "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
        "used_percentage = round(used_memory         /max_memory*100, 3)\n",
        "lora_percentage = round(used_memory_for_lora/max_memory*100, 3)\n",
        "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
        "print(f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\")\n",
        "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
        "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
        "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
        "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
      ],
      "metadata": {
        "id": "M7vzz-xeavwp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluate Fine-tuned Model\n",
        "\n",
        "These cells perform inference with the fine-tuned model and evaluate its performance on the test set.\n",
        "\n",
        "1. **Inference:** The first cell switches the model to inference mode and then iterates through the test images, extracting text using the model and storing the predictions.\n",
        "2. **Evaluation:** The second cell calculates the Word Error Rate (WER) and Character Error Rate (CER) by comparing the model's predictions to the ground truth (OCR output) for each test image. It then prints the average WER and CER, providing an overall assessment of the fine-tuned model's performance."
      ],
      "metadata": {
        "id": "IJh3U-khj2JJ"
      }
    },
    {
      "source": [
        "FastVisionModel.for_inference(model)  # Enable inference mode\n",
        "instruction = \"Extract the text from the given image.\"\n",
        "\n",
        "finetuned_predictions_test = {}\n",
        "for file in test_files:\n",
        "    try:\n",
        "        file_path = os.path.join(image_dir, file)\n",
        "        image = Image.open(file_path).convert(\"RGB\")\n",
        "        messages = [\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": [\n",
        "                    {\"type\": \"image\", \"image\": image},\n",
        "                    {\"type\": \"text\", \"text\": instruction}\n",
        "                ]\n",
        "            }\n",
        "        ]\n",
        "        input_text = tokenizer.apply_chat_template(messages, add_generation_prompt=True)\n",
        "        inputs = tokenizer(\n",
        "            image,\n",
        "            input_text,\n",
        "            add_special_tokens=False,\n",
        "            return_tensors=\"pt\"\n",
        "        ).to(\"cuda\")\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=128,\n",
        "            use_cache=True,\n",
        "            temperature=1.5,  # Consider adjusting\n",
        "            min_p=0.1        # Consider adjusting\n",
        "        )\n",
        "        pred_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        finetuned_predictions_test[file] = pred_text\n",
        "        print(f\"Fine-tuned inference processed for test image: {file}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error during inference for {file}: {e}\")"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "wvLborGTwzUc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "total_wer_test = 0.0\n",
        "total_cer_test = 0.0\n",
        "num_samples_test = 0\n",
        "\n",
        "for file, pred in finetuned_predictions_test.items():\n",
        "    ref = baseline_results.get(file, \"\")  # Assuming ground_truth_test is OCR output\n",
        "    error_wer = jiwer.wer(ref, pred)\n",
        "    error_cer = compute_cer(pred, ref)\n",
        "    print(f\"{file}: WER = {error_wer:.3f}, CER = {error_cer:.3f}\")\n",
        "    total_wer_test += error_wer\n",
        "    total_cer_test += error_cer\n",
        "    num_samples_test += 1\n",
        "\n",
        "if num_samples_test > 0:\n",
        "    print(\"\\n--- Overall Performance on Test Dataset (Fine-tuned) ---\")\n",
        "    print(f\"Average WER: {total_wer_test/num_samples_test:.3f}\")\n",
        "    print(f\"Average CER: {total_cer_test/num_samples_test:.3f}\")\n",
        "else:\n",
        "    print(\"No test samples processed.\")"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "zP5TyjBfw2p4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}